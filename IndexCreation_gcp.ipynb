{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":null,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":null,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":null,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":null,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":null,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[],"source":["spark"]},{"cell_type":"code","execution_count":null,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '206549784' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"multistre\" in b.name:\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":null,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n","doc_anchor_pairs = parquetFile.select(\"id\", \"anchor_text\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":null,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":null,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":null,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":null,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"72bcf46a"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":null,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["import numpy as np\n","from numpy.linalg import norm\n","from collections import Counter\n","from spark import Row\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token for token in tokens if token not in all_stopwords]\n","  word_count={}\n","  for token in tokens:\n","    if token not in word_count:\n","      word_count[token] = 1\n","    else:\n","      word_count[token] += 1\n","\n","  tokens_list = []\n","  for k in word_count.keys():\n","    temp = (k, (id, word_count[k]))\n","    tokens_list.append(temp)\n","  return tokens_list\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  sorted_pl = sorted(unsorted_pl, key = lambda x: x[0])\n","  return sorted_pl\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  counts = postings.map(lambda x: (x[0], len(x[1])))\n","  counts.groupByKey()\n","  return counts\n","\n","def partition_postings_and_write(postings, index_bins, folder_name):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","\n","  pl_to_block = postings.map(pl_bucket_mapper)\n","  pl_to_block = pl_to_block.reduceByKey(lambda a,b: [*a, *b])\n"," # join all the posting lists with the same bucket id to make it the same like the input of the write_a_posting_list method of an index opject\n"," # we do this in order to use the index to write the postings lists \n","  pl = pl_to_block.map(my_maper)\n","  my_index = InvertedIndex()\n","  posting_locs = pl.map(lambda b_w_pl: my_index.write_a_posting_list(b_w_pl, index_bins, folder_name))\n","  return posting_locs\n","\n","# def partition_postings_and_write(postings, index_bins, folder_name ):\n","#     postings = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","#     # write out all posting lists in a bucket to disk\n","#     postings = postings.groupByKey()\n","#     postings = postings.map(lambda x: InvertedIndex.write_a_posting_list(x,index_bins,folder_name))\n","#     # return the posting locations for each bucket\n","#     return postings\n","\n","def pl_bucket_mapper(item):\n","  return (token2bucket_id(item[0]), (item[0], item[1]))\n","\n","def my_maper(item):\n","  item1 = item[1]\n","  n = len(item1) - 1\n","  lst = [(item1[i], item1[i+1]) for i in range(0, n, 2)]\n","  \n","  return (item[0], lst)\n","\n","def doc_len(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token for token in tokens if token not in all_stopwords]\n","  return (id, len(tokens))\n","\n","\n","def doc_norm(text, id, d):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token for token in tokens if token not in all_stopwords]\n","  tokens_counter = Counter(tokens)\n","  vec = [item[1]/d[id] for item in tokens_counter.items() if d[id] != 0]\n","  \n","  \n","  return (id, norm(vec))\n","\n","def calculate_tf(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  counts = postings.map(lambda x: (x[0], map_tf(x[1])))             \n","  counts.groupByKey()\n","  return counts\n","\n","def map_tf(pr):\n","    tf = 0\n","    for p in pr:\n","        tf += p[1]\n","    return tf\n","\n","def map2(row):\n","    txt = \"\"\n","    anchors = row[1]\n","    id = row[0]\n","    \n","    for anchor in anchors:\n","        txt += anchor[1]\n","        txt += \" \"\n","    return Row(txt[:-1],id)"]},{"cell_type":"code","execution_count":null,"id":"4468eb03","metadata":{},"outputs":[],"source":["# text index\n","# word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","docs_len = doc_text_pairs.map(lambda x: doc_len(x[0], x[1]))\n","docs_dict = docs_len.collectAsMap()\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","w2tf = calculate_tf(postings_filtered)\n","w2tf_dict = w2tf.collectAsMap()\n","docs_norm = doc_text_pairs.map(lambda x: doc_norm(x[0], x[1], docs_dict))\n","norm_dict = docs_norm.collectAsMap()\n","\n","\n","# partition posting lists and write out\n","folder_name = \"postingText\"\n","_ = partition_postings_and_write(postings_filtered, bucket_name, folder_name).collect()\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postingText'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":null,"id":"a6131b51","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# Add the token - tf dictionary to the inverted index\n","inverted.term_total = w2tf_dict\n","# Add doc len dictionary to the inverted index\n","inverted.DL = docs_dict\n","inverted.docs_norm = norm_dict\n","# write the global stats out\n","inverted.write_index('.', 'text_index')\n","# upload to gs\n","index_src = \"text_index.pkl\"\n","index_dst = f'gs://{bucket_name}/postingText/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"dfbe0161","metadata":{},"outputs":[],"source":["# title index\n","# word counts map\n","word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","docs_len = doc_title_pairs.map(lambda x: doc_len(x[0], x[1]))\n","docs_dict = docs_len.collectAsMap()\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>0)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","w2tf = calculate_tf(postings_filtered)\n","w2tf_dict = w2tf.collectAsMap()\n","docs_norm = x.map(lambda x: doc_norm(x[0], x[1], docs_dict))\n","norm_dict = docs_norm.collectAsMap()\n","\n","\n","# partition posting lists and write out\n","folder_name = \"postings_title\"\n","_ = partition_postings_and_write(postings_filtered, bucket_name, folder_name).collect()\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_title'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# Add the token - tf dictionary to the inverted index\n","inverted.term_total = w2tf_dict\n","# Add doc len dictionary to the inverted index\n","inverted.DL = docs_dict\n","# write the global stats out\n","inverted.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_title/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"c9832af4","metadata":{},"outputs":[],"source":["anchor_docs = parquetFile.select(\"id\", \"anchor_text\").rdd.map(lambda r: (r[0], r[1]))\n","anchor = anchor_docs.flatMap(lambda x: x[1])\n","anchor1 = anchor.distinct().groupByKey().mapValues(lambda x: \" \".join(x))\n","anchor2 = anchor1.flatMap(lambda x: word_count(x[1], x[0]))\n","postings = anchor2.groupByKey().mapValues(reduce_word_counts)\n","folder_name = \"postings_anchor\"\n","_ = partition_postings_and_write(postings, bucket_name, folder_name).collect()\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_anchor'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","    \n","# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# write the global stats out\n","inverted.write_index('.', 'anchor_idx')\n","# upload to gs\n","index_src = \"anchor_idx.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_anchor/{index_src}'\n","!gsutil cp $index_src $index_dst"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}